{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# PyTorch Programming - Demonstrating Datasets\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide a few examples that show how PyTorch deals with datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "Let's start by importing what we need. [Torchvision](https://pytorch.org/vision/stable/index.html) is a very helpful that helps us deal with **vision** data a lot easier. There is also [Torchtext](https://pytorch.org/text/stable/index.html) and [Torchaudio](https://pytorch.org/audio/stable/index.html), which can help you with all kinds of [other types](https://pytorch.org/) of projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Xgr3ZNXmQx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KO552S5XtCH"
      },
      "source": [
        "In order to see how PyTorch can deal with datasets, we need a sample dataset to work with. [Torchvision](https://pytorch.org/vision/stable/index.html) offers a variety of built-in datasets itself that can easily work out of the box (https://pytorch.org/vision/stable/datasets.html) but here, we will be working with our own dataset:\n",
        "\n",
        "**[AckBinks: A Star Wars Dataset](https://github.com/atapour/dl-pytorch/tree/main/2.Datasets/AckBinks)**\n",
        "\n",
        "First, let's download the dataset, which is available on the GitHub repo that contains this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bGhK9wmXsA_"
      },
      "outputs": [],
      "source": [
        "!wget -q -O AckBinks.zip https://github.com/atapour/dl-pytorch/blob/main/2.Datasets/AckBinks/AckBinks.zip?raw=true\n",
        "!unzip -q AckBinks.zip\n",
        "!rm AckBinks.zip\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Q_hAATZQWb"
      },
      "source": [
        "Before we get to loading the data, we need to transform the data to make them suitable for PyTorch dataloaders. To do this, we will use torchvision [transforms](https://pytorch.org/vision/stable/transforms.html).\n",
        "\n",
        "In the following, we transform the training data to be all the same resolution (224x224), we make it so they are randomly flipped on their horizontal axis (as a form of data augmentation), we convert them to PyTorch Tensors and finally normalise their values, so that they are zero mean unit variance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTxY01eY0Aq"
      },
      "outputs": [],
      "source": [
        "train_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomResizedCrop(224),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "print('created transforms for training set!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "We shall do the same for our test data, though we need to be careful that we want the transforms applied to our test data to be as close as possible to real-world data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVqAtMBuw8LW"
      },
      "outputs": [],
      "source": [
        "test_transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(256),\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "print('created transforms for testing set!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "Now that our transforms are ready, we can create a [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) object for the test and training sets. Here, we use [ImageFolder](https://pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html) from Torchvision to create our dataset. Note that this does not actually load the dataset. The Dataset class basically creates a data structure telling us where the images actually are:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pou-TRizzZXP"
      },
      "outputs": [],
      "source": [
        "train_dataset = torchvision.datasets.ImageFolder('AckBinks/train', train_transform)\n",
        "test_dataset = torchvision.datasets.ImageFolder('AckBinks/test', test_transform)\n",
        "\n",
        "print(f\"There are {len(train_dataset)} images in the training set!\")\n",
        "print(f\"There are {len(test_dataset)} images in the test set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpff5vIw1vPb"
      },
      "source": [
        "We can have a look at the class names in our dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJSTSWit2D7I"
      },
      "outputs": [],
      "source": [
        "class_names = train_dataset.classes\n",
        "print(*class_names, sep = \", \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCWZ8Lf24i0"
      },
      "source": [
        "The [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) is responsible for actually loading the data. Note that since datasets can be massive, they are not meant to be loaded into memory all at once, which is why they are often loaded in \"*batch*\"es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrZ3qaos4str"
      },
      "source": [
        "In the following, we are going to load our data in batches of 4. We will shuffle the data during training and will disable the `shuffle` flag during test time.\n",
        "\n",
        "Note that `num_workers` refers to the number of threads, which can helps you process things faster if you have multiple processing cores available to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxus8583aX6"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "    batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "    batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyiYpF-44lw4"
      },
      "source": [
        "Let us now explore the data we have loaded. We can get the first batch from the `train_loader` iterable and look at the Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKGRvBn85m0K"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(train_loader))\n",
        "\n",
        "print(f\"Size of tensor: {x.shape}\")\n",
        "print(f\"Size of label: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Ej2Il75rFk"
      },
      "source": [
        "We can also try to visualise these images. We can do this using libraries like [tensorboardX](https://tensorboardx.readthedocs.io/en/latest/tensorboard.html), [OpenCV](https://opencv.org/), [Matplotlib](https://matplotlib.org/) or [Visdom](https://ai.facebook.com/tools/visdom/), but let's take this opportunity to take a look at [Weights and Biases](https://wandb.ai), which is becoming very popular and is commonly used in industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3uQp99QrkAW"
      },
      "source": [
        "If you havenâ€™t already, you will need to create a new account in order to be able to use Weights and Biases. It is free to use unless you are a massive corporation.\n",
        "\n",
        "You can visit [their website](https://wandb.ai/site) and sign up. It can then be simply installed using `pip` or `conda`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4SMkl90sV9O"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBSKuTiEsXKC"
      },
      "source": [
        "You will then need to authenticate yourself via the `wandb login` command. You will be prompted to copy and paste an authorisation key in order to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJqiocp56GGy"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlDo0ydkvc-f"
      },
      "source": [
        "We need to initialise a project:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRY8in65vcQZ"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=\"super-duper-demo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S8bzMK77Q21"
      },
      "source": [
        "Weights and Biases has many functionalities including experiment tracking, versioning, hyperparameter optimisation and others, but for now, we will use it as a simple visualisation tool.\n",
        "\n",
        "In the following, we create a table from our data where the columns are our images and their corresponding labels. We can then inspect the output in our dashboard. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT7DKnAs7axO"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(train_loader))\n",
        "\n",
        "columns = ['Image', 'Label']\n",
        "data = []\n",
        "\n",
        "for i, img in enumerate(x, 0):\n",
        "  data.append([wandb.Image(img), class_names[y[i].item()]])\n",
        "\n",
        "table = wandb.Table(data=data, columns=columns)\n",
        "wandb.log({\"AckBink Images\": table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrm2YerO0UxH"
      },
      "source": [
        "We can also try doing the same thing like a champ, without any fancy packages.\n",
        "\n",
        "Torchvision can help us create a grid of our images, which we can then display using Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHarcDHiBEw1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x, y = next(iter(train_loader))\n",
        "\n",
        "grid = torchvision.utils.make_grid(x)\n",
        "grid = (grid-grid.min())/(grid.max()-grid.min())\n",
        "\n",
        "plt.imshow(grid.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt6qM7KG9IHk"
      },
      "source": [
        "Torchvision also has numerous datasets, often used for research and benchmarking, built into it.\n",
        "\n",
        "[https://pytorch.org/vision/stable/datasets.html](https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        "\n",
        "These built-in datasets are very easy to load and work with. For example, let's look at the seminal MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b_6cyPX_mLx"
      },
      "outputs": [],
      "source": [
        "# MNIST example\n",
        "mnist_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
        "        torchvision.transforms.Resize(32),\n",
        "        torchvision.transforms.ToTensor()\n",
        "    ])),\n",
        "shuffle=True, batch_size=64, drop_last=True)\n",
        "print('MINST has been loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avSZOMcwDHBE"
      },
      "source": [
        "Let's have a look at some of the images in the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac6uBi_CDmux"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(mnist_loader))\n",
        "\n",
        "grid = torchvision.utils.make_grid(x)\n",
        "grid = (grid-grid.min())/(grid.max()-grid.min())\n",
        "\n",
        "print(f'Size of image is {x.size()}')\n",
        "plt.imshow(grid.permute(1, 2, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9cR6mthESaV"
      },
      "source": [
        "There are various dataset processing tools available in PyTorch, which can help you load in and pre-process your data easily in efficient ways.\n",
        "\n",
        "https://pytorch.org/vision/stable/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------\n",
        "\n",
        "Copyright (c) 2023 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "based on https://github.com/cwkx/ml-materials\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2.PyTorch_Programming_Datasets.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('3.9.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
