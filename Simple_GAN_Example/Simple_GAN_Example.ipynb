{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# Deep Learning - A Simple GAN Example in PyTorch\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide an example that shows the implementation of a simple Generative Adversarial Network (GAN) in PyTorch.\n",
        "\n",
        "Copyright (c) 2023 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "We are going to implement a simple GAN. Let's start by importing what we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Xgr3ZNXmQx",
        "outputId": "c20d0c8f-cc2d-409a-b664-50cd7451e941"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Device is {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZr-PApBcN4Y"
      },
      "source": [
        "We should now line up the dataset we are going to use. We will be working with our own dataset:\n",
        "\n",
        "**[AckBinks: A Star Wars Dataset](https://github.com/atapour/dl-pytorch/tree/main/2.Datasets/AckBinks)**\n",
        "\n",
        "First, let's download the dataset, which is publicly available on GitHub: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bGhK9wmXsA_",
        "outputId": "1942e2db-d2c8-4b00-8d5b-7b9382c94a4f"
      },
      "outputs": [],
      "source": [
        "!wget -q -O AckBinks.zip https://github.com/atapour/dl-pytorch/blob/main/2.Datasets/AckBinks/AckBinks.zip?raw=true\n",
        "!unzip -q AckBinks.zip\n",
        "!rm AckBinks.zip\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6AZQ9Kq_1hj"
      },
      "source": [
        "Now we are ready to process our data. We are going to convert our data to 32x32 grayscale images to make the work easier and more efficient just for demonstration purposes.\n",
        "\n",
        "Since our dataset has two classes, we are also going to pick one of these classes and have our GAN generate images of that class only. Let's pick Jar Jar Binks since he is better-looking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbTxY01eY0Aq",
        "outputId": "74a6a4f7-6d58-4dbe-f89e-50c7b0862450"
      },
      "outputs": [],
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize([32,32]),\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder('AckBinks/train', transform)\n",
        "\n",
        "# picking Jar Jar Binks as the class to generate:\n",
        "idx = torch.tensor(dataset.targets) == 1\n",
        "\n",
        "train_dataset = torch.utils.data.dataset.Subset(dataset, np.where(idx == 1)[0])\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "    batch_size=8, shuffle=True, num_workers = 2)\n",
        "\n",
        "print(f\"There are {len(train_dataset)} images in the training set!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "Our dataset is tiny and is not really suited for any real applications, but it will demonstrate the process. Let's look at a few of our images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "tVqAtMBuw8LW",
        "outputId": "360c9049-5d3e-44d2-802f-fbb3c60fc8e1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_loader.dataset[i][0].clamp(0,1).permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "We can now create our models, we need a Generator and a Discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pou-TRizzZXP",
        "outputId": "17cc4450-481c-4a35-dda6-df90782d387a"
      },
      "outputs": [],
      "source": [
        "# A few parameters:\n",
        "n_channels = 3\n",
        "img_width = 32\n",
        "\n",
        "# define the generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_size=100):\n",
        "        super(Generator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(latent_size, 128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1024, n_channels*img_width*img_width),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = x.view(x.size(0), n_channels, img_width, img_width)\n",
        "        return x\n",
        "\n",
        "# define the discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(n_channels*img_width*img_width, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1), # 1 output for real/fake\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.layer(x)\n",
        "        return x\n",
        "        \n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "print(f'Generator has {len(torch.nn.utils.parameters_to_vector(G.parameters()))} parameters.')\n",
        "print(f'Discriminator has {len(torch.nn.utils.parameters_to_vector(D.parameters()))} parameters')\n",
        "\n",
        "# initialise the optimiser\n",
        "optimiser_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimiser_D = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "print('Optimisers have been created!')\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "epoch = 0\n",
        "print('Loss function is Binary Cross Entropy!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDngw8EueCv"
      },
      "source": [
        "Let's start the main training loop now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ccMuktrKukIj",
        "outputId": "8db504b2-4a24-4fd7-ef29-cf4469f4e837"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "while (epoch<20000):\n",
        "    \n",
        "    # arrays for metrics\n",
        "    logs = {}\n",
        "    gen_loss_arr = np.zeros(0)\n",
        "    dis_loss_arr = np.zeros(0)\n",
        "\n",
        "    # iterate over the train dateset\n",
        "    for i, batch in enumerate(train_loader):\n",
        "\n",
        "        x, t = batch\n",
        "        x, t = x.to(device), t.to(device)\n",
        "\n",
        "        # train discriminator \n",
        "        g = G(torch.randn(x.size(0), 100).to(device))\n",
        "        l_r = criterion(D(x).mean(), torch.ones(1)[0].to(device)) # real -> 1\n",
        "        l_f = criterion(D(g.detach()).mean(), torch.zeros(1)[0].to(device)) #  fake -> 0\n",
        "        loss_d = (l_r + l_f)/2.0\n",
        "        optimiser_D.zero_grad()\n",
        "        loss_d.backward()\n",
        "        optimiser_D.step()\n",
        "        \n",
        "        # train generator\n",
        "        g = G(torch.randn(x.size(0), 100).to(device))\n",
        "        loss_g = criterion(D(g).mean(), torch.ones(1)[0].to(device)) # fake -> 1\n",
        "        optimiser_G.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimiser_G.step()\n",
        "\n",
        "        gen_loss_arr = np.append(gen_loss_arr, loss_g.item())\n",
        "        dis_loss_arr = np.append(dis_loss_arr, loss_d.item())\n",
        "\n",
        "    # plot some examples\n",
        "    G.eval()\n",
        "    g = G(torch.randn(x.size(0), 100).to(device))\n",
        "    print('loss D: {:.3f}, loss G: {:.3f}'.format(gen_loss_arr.mean(), dis_loss_arr.mean()))\n",
        "    plt.grid(False)\n",
        "    plt.imshow(torchvision.utils.make_grid(g).cpu().data.clamp(0,1).permute(0,2,1).contiguous().permute(2,1,0), cmap=plt.cm.binary)\n",
        "    plt.show()\n",
        "    plt.pause(0.0001)\n",
        "    G.train()\n",
        "\n",
        "    epoch = epoch+1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Simple_GAN_Example.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('3.9.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
