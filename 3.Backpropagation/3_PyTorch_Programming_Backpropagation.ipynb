{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# PyTorch Programming - Demonstrating Backpropagation\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide a few examples that show backpropagation in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "We are going to look at how PyTorch supports backpropagation. Let's start by creating a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Xgr3ZNXmQx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "x = torch.zeros(4, 4)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZr-PApBcN4Y"
      },
      "source": [
        "In \"deep learning\", we would like to be able to change the values of the parameters of our network as we train it so that the output of the network gets closer and closer to the ground truth.\n",
        "\n",
        "In this vein, we want to calculate the gradient of the loss function with respect to the parameters, so we can change the parameters accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KO552S5XtCH"
      },
      "source": [
        "Every tensor has a [`requires_grad`](https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html), which is `false` by default. This means new pytorch tensors are little more than numpy arrays at first. Let's confirm this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bGhK9wmXsA_"
      },
      "outputs": [],
      "source": [
        "print(x.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1Q_hAATZQWb"
      },
      "source": [
        "If we perform an operation on our tensor `x` and inspect its gradient, we will find that there in nothing there by default:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTxY01eY0Aq"
      },
      "outputs": [],
      "source": [
        "z = x + 2\n",
        "print(z)\n",
        "\n",
        "# without setting requires_grad, the tensor has no gradient:\n",
        "\n",
        "print(f' The gradient function is {z.grad_fn}')   # expected to be empty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "Let's start again and this time make sure `requires_grad` is set to see what we can do.\n",
        "\n",
        "We will delete `x` to start over cleanly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVqAtMBuw8LW"
      },
      "outputs": [],
      "source": [
        "del x\n",
        "x = torch.zeros(4,4)\n",
        "x.requires_grad = True\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "Now, we create `z` again and see what happens when `x` has the `requires_grad` set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pou-TRizzZXP"
      },
      "outputs": [],
      "source": [
        "z = x + 2\n",
        "print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDngw8EueCv"
      },
      "source": [
        "Under the hood, a computational graph is being created as tensors undergo operations. This graph keeps track of every operation that is applied to the data.\n",
        "\n",
        "We can see that the `grad_fn` is keeping track of the operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccMuktrKukIj"
      },
      "outputs": [],
      "source": [
        "print(f'z.grad_fn: {z.grad_fn}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEDUkoziuwUv"
      },
      "source": [
        "Let's take one more step and check the gradient function again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51-x09MOumTl"
      },
      "outputs": [],
      "source": [
        "z = z * 5\n",
        "print(f'z.grad_fn: {z.grad_fn}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25GE0Jbxu-ak"
      },
      "source": [
        "We can even walk back through the graph and see the previous operations (not that we ever need to do that in the wild):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJSTSWit2D7I"
      },
      "outputs": [],
      "source": [
        "print(z.grad_fn.next_functions[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCWZ8Lf24i0"
      },
      "source": [
        "Imagine now that `z` is the output of our neural network. We thus want to have the output be as close as possible to the ground truth. We then need to calculate a loss function, which measures the distance between the output and the ground truth, and subsequently minimise this loss.\n",
        "\n",
        "Let's first create a hypothetical ground truth, a tensor, the same size as `z` with the value of all ones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpxus8583aX6"
      },
      "outputs": [],
      "source": [
        "# ground truth:\n",
        "gt = torch.ones_like(z)\n",
        "print(gt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyiYpF-44lw4"
      },
      "source": [
        "We can have the loss be the mean squared error between the output, `z`, and the ground truth, `gt`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKGRvBn85m0K"
      },
      "outputs": [],
      "source": [
        "loss = ((z - gt)**2).mean()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Ej2Il75rFk"
      },
      "source": [
        "This `loss` should be a single value, which tells us how good or bad the prediction of our neural network has been and we want to minimise this loss to have the output of the our network be close to the ground truth.\n",
        "\n",
        "Let's look at the gradient function of `loss`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4SMkl90sV9O"
      },
      "outputs": [],
      "source": [
        "print(loss.grad_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBSKuTiEsXKC"
      },
      "source": [
        "Now is the time for **backpropagation**.\n",
        "\n",
        "The function `.backward()` starts from the `loss` variables and moves \"backward\" through the computational graph and computes the gradients of the `loss` with respect to all the parameters in our neural network. These gradients are then used to change the parameters in the direction of a better output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJqiocp56GGy"
      },
      "outputs": [],
      "source": [
        "loss.backward()\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlDo0ydkvc-f"
      },
      "source": [
        "If we look at the gradient of the input, `x`, we know how much to change the parameters to minimise the overall loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRY8in65vcQZ"
      },
      "outputs": [],
      "source": [
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S8bzMK77Q21"
      },
      "source": [
        "This tool set makes training neural networks very simple and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----------\n",
        "\n",
        "Copyright (c) 2023 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "based on https://github.com/cwkx/ml-materials\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3.PyTorch_Programming_Backpropagation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit ('3.9.13')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
