{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DrpK6lIWrhi"
      },
      "source": [
        "# Deep Learning - RNN Sentiment Analysis\n",
        "---\n",
        "\n",
        "## Author : Amir Atapour-Abarghouei, amir.atapour-abarghouei@durham.ac.uk\n",
        "\n",
        "This notebook will provide an example that shows the implementation of a simple RNN for sentiment analysis in PyTorch.\n",
        "\n",
        "Copyright (c) 2023 Amir Atapour-Abarghouei, UK.\n",
        "\n",
        "License : LGPL - http://www.gnu.org/licenses/lgpl.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8-07GGp-fMw"
      },
      "source": [
        "Since [TorchData](https://github.com/pytorch/data) depends on nightly builds of PyTorch, to avoid any versioning issues, we will not be using [TorchText](https://pytorch.org/text/stable/index.html), and we'll be performing the data processing manually, but outside the Google Colab environment, you can use TorchText, which is clear and much better and has removed a lot of unnecessary abstraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x85uyaEzXw4"
      },
      "source": [
        "We are going take advantage of the power of RNNs to build a model that is capable of detecting sentiment (i.e., whether a sentence is positive or negative) using PyTorch. This will be done on movie reviews, using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-exrFDujXlUG"
      },
      "source": [
        "Let's start by importing what we need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Xgr3ZNXmQx",
        "outputId": "32405ef8-69a1-4532-d813-e7c10560bb7c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import tarfile\n",
        "import requests\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(f'torch version: {torch.__version__}')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Device is {device}!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZr-PApBcN4Y"
      },
      "source": [
        "Now, we are going to download and unpack the dataset directly from the source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbTxY01eY0Aq",
        "outputId": "f23cf071-d5eb-4a94-8375-e830e96a4a08"
      },
      "outputs": [],
      "source": [
        "# Download IMDb dataset\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "r = requests.get(url)\n",
        "with open('aclImdb_v1.tar.gz', 'wb') as file:\n",
        "    file.write(r.content)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UuxfuuhpLIN"
      },
      "source": [
        "Now, we process the data and prepare them to be used. The following code block defines two functions, `preprocess_text` and `load_data`, to process text data. `preprocess_text` converts text to lowercase, removes punctuation, and splits it into words, while `load_data` reads text files from specified directories, applies preprocessing, and labels them as positive (1) or negative (0), creating a dataset for sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2NscGlApLph",
        "outputId": "2bfdf6b9-d23a-48cb-bb55-63ec36bdc2b1"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(f'[{string.punctuation}]', '', text)\n",
        "    return text.split()\n",
        "\n",
        "def load_data(path):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        directory = f'{path}/{label}'\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith('.txt'):\n",
        "                with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
        "                    text = file.read()\n",
        "                    text = preprocess_text(text)\n",
        "                    data.append((1 if label == 'pos' else 0, text))\n",
        "    return data\n",
        "\n",
        "train_data = load_data('aclImdb/train')\n",
        "test_data = load_data('aclImdb/test')\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5UnQLwJZW6O"
      },
      "source": [
        "Now, we can get the dataset, which is already built into TorchText. The following command will download the IMDB dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVqAtMBuw8LW",
        "outputId": "6c5da8ab-d570-456c-efb3-1f2f355789d9"
      },
      "outputs": [],
      "source": [
        "print(f'\\nThere are {len(train_data)} data points in the training set!')\n",
        "print(f'There are {len(test_data)} data points in the testing set!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt8tSeQcy_jZ"
      },
      "source": [
        "Let's check one of the data samples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pou-TRizzZXP",
        "outputId": "c8aa931c-7194-4588-c23c-ffe1296a22df"
      },
      "outputs": [],
      "source": [
        "print(' '.join(train_data[0][1]))\n",
        "print(f'label: {train_data[0][0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBDngw8EueCv"
      },
      "source": [
        "Now is the time to build a vocabulary. The following block of code creates a vocabulary mapping words to unique indices from our dataset, assigning special indices for unknown and padding tokens, and defines a function to convert text data into fixed-length sequences of these indices, padding shorter sequences and truncating longer ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccMuktrKukIj",
        "outputId": "d01ca096-f553-4e26-a9b1-872883839de5"
      },
      "outputs": [],
      "source": [
        "# Build vocabulary\n",
        "word_counts = Counter()\n",
        "for _, text in train_data:\n",
        "    word_counts.update(text)\n",
        "\n",
        "vocab = {word: i+2 for i, (word, _) in enumerate(word_counts.most_common())}\n",
        "vocab['<unk>'] = 1\n",
        "vocab['<pad>'] = 0\n",
        "\n",
        "# Encode and pad data\n",
        "def encode_and_pad(data, vocab, max_length=500):\n",
        "    encoded_data = []\n",
        "    labels = []\n",
        "    for label, text in data:\n",
        "        encoded_text = [vocab.get(word, vocab['<unk>']) for word in text][:max_length]\n",
        "        padded_text = encoded_text + [vocab['<pad>']] * (max_length - len(encoded_text))\n",
        "        encoded_data.append(padded_text)\n",
        "        labels.append(label)\n",
        "    return np.array(encoded_data), np.array(labels)\n",
        "\n",
        "X_train, y_train = encode_and_pad(train_data, vocab)\n",
        "X_test, y_test = encode_and_pad(test_data, vocab)\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB1Vmy4OHYyN"
      },
      "source": [
        "Note that we also have the `<unk>` and `<pad>` tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98c2vQgaJAyb"
      },
      "source": [
        "Now we make a class to handle our dataset cleanly. The following block of code defines the `IMDBDataset` class, a custom dataset for handling our IMDb movie review data, and creates instances for training and testing data. It also sets up `DataLoader` objects for both datasets, which facilitate efficient batching, shuffling, and loading of the data during model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCm-JMNCJlzb",
        "outputId": "8110fe77-62af-4e3d-a885-e6c018d3bb5e"
      },
      "outputs": [],
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])\n",
        "\n",
        "train_dataset = IMDBDataset(X_train, y_train)\n",
        "test_dataset = IMDBDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "print('The data is ready!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Usd9bZHKuQG"
      },
      "source": [
        "Now, we will create our optimiser and our network, a very simple architecture made up of an embedding layer, an RNN layer and a linear layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3o9_dMyK3qL",
        "outputId": "aa1c3ed5-7038-406f-f3a1-01afd529d375"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)  # Added batch_first=True\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch size, seq len]\n",
        "        embedded = self.embedding(text)  # embedded: [batch size, seq len, emb dim]\n",
        "        output, hidden = self.rnn(embedded)  # output: [batch size, seq len, hidden dim], hidden: [1, batch size, hidden dim]\n",
        "        # Use the hidden state from the last layer\n",
        "        hidden = hidden[-1]  # hidden: [batch size, hidden dim]\n",
        "        return self.fc(hidden)\n",
        "\n",
        "\n",
        "# a few parameters to set for our model:\n",
        "input_dim = len(vocab)\n",
        "embed_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "\n",
        "model = RNN(input_dim, embed_dim, hidden_dim, output_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "print(model)\n",
        "print('Model has been created!')\n",
        "print(f'Model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} parameters!')\n",
        "\n",
        "# create the optimiser:\n",
        "optimiser = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "# loss function:\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Q2w3CfRk4_"
      },
      "source": [
        "In order to keep track of how our model performs we need to calculate the accuracy of our predictions. This helper function will help us computer accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu5l408MR5JW",
        "outputId": "2b106310-8c16-4422-c07f-ddab58dd1350"
      },
      "outputs": [],
      "source": [
        "# Function to calculate accuracy:\n",
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "print('done!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogjYCWEZPS85"
      },
      "source": [
        "And now the main training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        },
        "id": "9U29sw7SQd4f",
        "outputId": "621c393d-6470-4597-9a15-02d3d59a049e"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()  # Set the model to training mode\n",
        "    for texts, labels in train_loader:\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(texts)\n",
        "\n",
        "        # Ensure predictions are the right shape\n",
        "        predictions = predictions.squeeze()  # Remove any singleton dimensions\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        # Calculate accuracy\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        epoch_loss += loss.item()/len(train_loader)\n",
        "        epoch_acc += acc.item()/len(train_loader)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} || Train Loss: {epoch_loss:.3f} - Train Acc: {epoch_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va3BrLU3ZdOp"
      },
      "source": [
        "Now that the training is complete, we should evaluate the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oThHa11tZkmU",
        "outputId": "832356e6-80e7-42c2-9f74-09ade23c5d9f"
      },
      "outputs": [],
      "source": [
        "eval_loss = 0\n",
        "eval_acc = 0\n",
        "\n",
        "# Evaluation loop (example)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        texts, labels = batch\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "\n",
        "        predictions = model(texts)\n",
        "        predictions = predictions.squeeze()  \n",
        "\n",
        "        # calculate loss and accuracy:\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "        eval_loss += loss.item() / len(test_loader)\n",
        "        eval_acc += acc.item() / len(test_loader)\n",
        "\n",
        "print(f'Test Loss: {eval_loss:.3f} - Test Acc: {eval_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPcjj0YxbVo4"
      },
      "source": [
        "Don't be surprised, the results are awful. There are lots of ways to improve what we have done here. Try to see if you can improve things."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "fff836058c255167b80f2d77a2226e7e00ff1ecf6518b7f3cd25e9b70384f747"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
